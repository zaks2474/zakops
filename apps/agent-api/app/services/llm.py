"""LLM service for managing LLM calls with retries and fallback mechanisms."""

from typing import (
    Any,
    Dict,
    List,
    Optional,
)

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from openai import (
    APIError,
    APITimeoutError,
    OpenAIError,
    RateLimitError,
)
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from app.core.config import (
    Environment,
    settings,
)
from app.core.logging import logger


def _create_llm_instance(
    model: str,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    **kwargs
) -> ChatOpenAI:
    """Create an LLM instance with vLLM or OpenAI backend.

    Per Decision Lock: Local vLLM is primary inference lane.
    Falls back to OpenAI if VLLM_BASE_URL is not set.
    """
    llm_kwargs = {
        "model": model,
        "max_tokens": max_tokens or settings.MAX_TOKENS,
    }

    if temperature is not None:
        llm_kwargs["temperature"] = temperature

    # Use vLLM if configured (Decision Lock: local-first)
    if settings.VLLM_BASE_URL:
        llm_kwargs["base_url"] = settings.VLLM_BASE_URL
        llm_kwargs["api_key"] = "not-needed"  # vLLM doesn't require API key
        logger.debug("using_vllm_backend", base_url=settings.VLLM_BASE_URL, model=model)
    elif settings.OPENAI_API_KEY:
        llm_kwargs["api_key"] = settings.OPENAI_API_KEY
        logger.debug("using_openai_backend", model=model)
    else:
        # No backend configured - will fail at runtime
        logger.warning("no_llm_backend_configured", model=model)
        llm_kwargs["api_key"] = "not-configured"

    llm_kwargs.update(kwargs)
    return ChatOpenAI(**llm_kwargs)


class LLMRegistry:
    """Registry of available LLM models with pre-initialized instances.

    This class maintains a list of LLM configurations and provides
    methods to retrieve them by name with optional argument overrides.

    Per Decision Lock: Primary inference is local vLLM (Qwen2.5-32B-Instruct-AWQ).
    OpenAI models are fallback only.
    """

    # Class-level variable containing all available LLM models
    # Lazy initialization to allow config to load first
    _llms: Optional[List[Dict[str, Any]]] = None

    @classmethod
    def _init_llms(cls) -> List[Dict[str, Any]]:
        """Initialize LLM instances lazily."""
        if cls._llms is not None:
            return cls._llms

        llms = []

        # Add vLLM models if configured (primary per Decision Lock)
        if settings.VLLM_BASE_URL:
            llms.extend([
                {
                    "name": "Qwen/Qwen2.5-32B-Instruct-AWQ",
                    "llm": _create_llm_instance(
                        model="Qwen/Qwen2.5-32B-Instruct-AWQ",
                        temperature=settings.DEFAULT_LLM_TEMPERATURE,
                    ),
                },
                {
                    "name": "Qwen/Qwen2.5-7B-Instruct-AWQ",
                    "llm": _create_llm_instance(
                        model="Qwen/Qwen2.5-7B-Instruct-AWQ",
                        temperature=settings.DEFAULT_LLM_TEMPERATURE,
                    ),
                },
            ])

        # Add OpenAI models if API key available (fallback)
        if settings.OPENAI_API_KEY:
            llms.extend([
                {
                    "name": "gpt-4o",
                    "llm": _create_llm_instance(
                        model="gpt-4o",
                        temperature=settings.DEFAULT_LLM_TEMPERATURE,
                        top_p=0.95 if settings.ENVIRONMENT == Environment.PRODUCTION else 0.8,
                        presence_penalty=0.1 if settings.ENVIRONMENT == Environment.PRODUCTION else 0.0,
                        frequency_penalty=0.1 if settings.ENVIRONMENT == Environment.PRODUCTION else 0.0,
                    ),
                },
                {
                    "name": "gpt-4o-mini",
                    "llm": _create_llm_instance(
                        model="gpt-4o-mini",
                        temperature=settings.DEFAULT_LLM_TEMPERATURE,
                        top_p=0.9 if settings.ENVIRONMENT == Environment.PRODUCTION else 0.8,
                    ),
                },
            ])

        # Fallback: create a placeholder that will fail at runtime with clear error
        if not llms:
            logger.error("no_llm_backend_configured",
                        hint="Set VLLM_BASE_URL for local inference or OPENAI_API_KEY for cloud")
            llms.append({
                "name": "unconfigured",
                "llm": _create_llm_instance(model="unconfigured"),
            })

        cls._llms = llms
        return llms

    @classmethod
    @property
    def LLMS(cls) -> List[Dict[str, Any]]:
        """Get the list of available LLMs (lazy initialization)."""
        return cls._init_llms()

    @classmethod
    def get(cls, model_name: str, **kwargs) -> BaseChatModel:
        """Get an LLM by name with optional argument overrides.

        Args:
            model_name: Name of the model to retrieve
            **kwargs: Optional arguments to override default model configuration

        Returns:
            BaseChatModel instance

        Raises:
            ValueError: If model_name is not found in LLMS
        """
        llms = cls._init_llms()

        # Find the model in the registry
        model_entry = None
        for entry in llms:
            if entry["name"] == model_name:
                model_entry = entry
                break

        if not model_entry:
            available_models = [entry["name"] for entry in llms]
            raise ValueError(
                f"model '{model_name}' not found in registry. available models: {', '.join(available_models)}"
            )

        # If user provides kwargs, create a new instance with those args
        if kwargs:
            logger.debug("creating_llm_with_custom_args", model_name=model_name, custom_args=list(kwargs.keys()))
            return _create_llm_instance(model=model_name, **kwargs)

        # Return the default instance
        logger.debug("using_default_llm_instance", model_name=model_name)
        return model_entry["llm"]

    @classmethod
    def get_all_names(cls) -> List[str]:
        """Get all registered LLM names in order.

        Returns:
            List of LLM names
        """
        return [entry["name"] for entry in cls._init_llms()]

    @classmethod
    def get_model_at_index(cls, index: int) -> Dict[str, Any]:
        """Get model entry at specific index.

        Args:
            index: Index of the model in LLMS list

        Returns:
            Model entry dict
        """
        llms = cls._init_llms()
        if 0 <= index < len(llms):
            return llms[index]
        return llms[0]  # Wrap around to first model


class LLMService:
    """Service for managing LLM calls with retries and circular fallback.

    This service handles all LLM interactions with automatic retry logic,
    rate limit handling, and circular fallback through all available models.
    """

    def __init__(self):
        """Initialize the LLM service."""
        self._llm: Optional[BaseChatModel] = None
        self._current_model_index: int = 0

        # Find index of default model in registry
        all_names = LLMRegistry.get_all_names()
        try:
            self._current_model_index = all_names.index(settings.DEFAULT_LLM_MODEL)
            self._llm = LLMRegistry.get(settings.DEFAULT_LLM_MODEL)
            logger.info(
                "llm_service_initialized",
                default_model=settings.DEFAULT_LLM_MODEL,
                model_index=self._current_model_index,
                total_models=len(all_names),
                environment=settings.ENVIRONMENT.value,
            )
        except (ValueError, Exception) as e:
            # Default model not found, use first model
            self._current_model_index = 0
            self._llm = LLMRegistry._init_llms()[0]["llm"]
            logger.warning(
                "default_model_not_found_using_first",
                requested=settings.DEFAULT_LLM_MODEL,
                using=all_names[0] if all_names else "none",
                error=str(e),
            )

    def _get_next_model_index(self) -> int:
        """Get the next model index in circular fashion.

        Returns:
            Next model index (wraps around to 0 if at end)
        """
        total_models = len(LLMRegistry._init_llms())
        next_index = (self._current_model_index + 1) % total_models
        return next_index

    def _switch_to_next_model(self) -> bool:
        """Switch to the next model in the registry (circular).

        Returns:
            True if successfully switched, False otherwise
        """
        try:
            next_index = self._get_next_model_index()
            next_model_entry = LLMRegistry.get_model_at_index(next_index)

            logger.warning(
                "switching_to_next_model",
                from_index=self._current_model_index,
                to_index=next_index,
                to_model=next_model_entry["name"],
            )

            self._current_model_index = next_index
            self._llm = next_model_entry["llm"]

            logger.info("model_switched", new_model=next_model_entry["name"], new_index=next_index)
            return True
        except Exception as e:
            logger.error("model_switch_failed", error=str(e))
            return False

    @retry(
        stop=stop_after_attempt(settings.MAX_LLM_CALL_RETRIES),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIError)),
        before_sleep=before_sleep_log(logger, "WARNING"),
        reraise=True,
    )
    async def _call_llm_with_retry(self, messages: List[BaseMessage]) -> BaseMessage:
        """Call the LLM with automatic retry logic.

        Args:
            messages: List of messages to send to the LLM

        Returns:
            BaseMessage response from the LLM

        Raises:
            OpenAIError: If all retries fail
        """
        if not self._llm:
            raise RuntimeError("llm not initialized")

        try:
            response = await self._llm.ainvoke(messages)
            logger.debug("llm_call_successful", message_count=len(messages))
            return response
        except (RateLimitError, APITimeoutError, APIError) as e:
            logger.warning(
                "llm_call_failed_retrying",
                error_type=type(e).__name__,
                error=str(e),
                exc_info=True,
            )
            raise
        except OpenAIError as e:
            logger.error(
                "llm_call_failed",
                error_type=type(e).__name__,
                error=str(e),
            )
            raise

    async def call(
        self,
        messages: List[BaseMessage],
        model_name: Optional[str] = None,
        **model_kwargs,
    ) -> BaseMessage:
        """Call the LLM with the specified messages and circular fallback.

        Args:
            messages: List of messages to send to the LLM
            model_name: Optional specific model to use. If None, uses current model.
            **model_kwargs: Optional kwargs to override default model configuration

        Returns:
            BaseMessage response from the LLM

        Raises:
            RuntimeError: If all models fail after retries
        """
        # If user specifies a model, get it from registry
        if model_name:
            try:
                self._llm = LLMRegistry.get(model_name, **model_kwargs)
                # Update index to match the requested model
                all_names = LLMRegistry.get_all_names()
                try:
                    self._current_model_index = all_names.index(model_name)
                except ValueError:
                    pass  # Keep current index if model name not in list
                logger.info("using_requested_model", model_name=model_name, has_custom_kwargs=bool(model_kwargs))
            except ValueError as e:
                logger.error("requested_model_not_found", model_name=model_name, error=str(e))
                raise

        # Track which models we've tried to prevent infinite loops
        total_models = len(LLMRegistry._init_llms())
        models_tried = 0
        starting_index = self._current_model_index
        last_error = None

        while models_tried < total_models:
            try:
                response = await self._call_llm_with_retry(messages)
                return response
            except OpenAIError as e:
                last_error = e
                models_tried += 1

                current_model_name = LLMRegistry._init_llms()[self._current_model_index]["name"]
                logger.error(
                    "llm_call_failed_after_retries",
                    model=current_model_name,
                    models_tried=models_tried,
                    total_models=total_models,
                    error=str(e),
                )

                # If we've tried all models, give up
                if models_tried >= total_models:
                    logger.error(
                        "all_models_failed",
                        models_tried=models_tried,
                        starting_model=LLMRegistry._init_llms()[starting_index]["name"],
                    )
                    break

                # Switch to next model in circular fashion
                if not self._switch_to_next_model():
                    logger.error("failed_to_switch_to_next_model")
                    break

                # Continue loop to try next model

        # All models failed
        raise RuntimeError(
            f"failed to get response from llm after trying {models_tried} models. last error: {str(last_error)}"
        )

    def get_llm(self) -> Optional[BaseChatModel]:
        """Get the current LLM instance.

        Returns:
            Current BaseChatModel instance or None if not initialized
        """
        return self._llm

    def bind_tools(self, tools: List) -> "LLMService":
        """Bind tools to the current LLM.

        Args:
            tools: List of tools to bind

        Returns:
            Self for method chaining
        """
        if self._llm:
            self._llm = self._llm.bind_tools(tools)
            logger.debug("tools_bound_to_llm", tool_count=len(tools))
        return self


# Create global LLM service instance
llm_service = LLMService()
